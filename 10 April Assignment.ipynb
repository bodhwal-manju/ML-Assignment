{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cc180b1-e26a-45dc-8748-84216c3150fa",
   "metadata": {},
   "source": [
    "# Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9858a53-7b71-4c02-9142-28217bfecb62",
   "metadata": {},
   "source": [
    "Ans: We can solve this problem using Bayes' theorem. Let's define the following events:\n",
    "\n",
    "1. A: the employee uses the health insurance plan\n",
    "2. B: the employee is a smoker\n",
    "We are given the following probabilities:\n",
    "\n",
    "P(A) = 0.7 (70% of the employees use the company's health insurance plan) P(B|A) = 0.4 (40% of the employees who use the plan are smokers)\n",
    "\n",
    "We want to find P(B|A), the probability that an employee is a smoker given that he/she uses the health insurance plan.\n",
    "\n",
    "Using Bayes' theorem, we have:\n",
    "\n",
    "P(B|A) = P(A|B) * P(B) / P(A)\n",
    "\n",
    "We can find each of these probabilities as follows:\n",
    "\n",
    "P(B) = the overall probability of being a smoker, which we are not given directly. However, we can find it using the law of total probability:\n",
    "\n",
    "P(B) = P(B|A) * P(A) + P(B|not A) * P(not A)\n",
    "\n",
    "We are given P(B|A) = 0.4 and P(A) = 0.7, and we can assume that P(B|not A) is the proportion of employees who are smokers but do not use the health insurance plan. We are not given this proportion, but we can assume it is smaller than 40%, since the use of health insurance plan is likely to be higher among smokers.\n",
    "        \n",
    "Let's assume P(B|not A) = 0.2, which means that 20% of the employees who do not use the plan are smokers. Then:\n",
    "\n",
    "P(B) = 0.4 * 0.7 + 0.2 * 0.3 = 0.34\n",
    "\n",
    "Next, we need to find P(A|B), the probability that an employee uses the health insurance plan given that he/she is a smoker. We can assume that this proportion is higher than the overall proportion of 70%, since smokers are more likely to use health insurance. Let's assume P(A|B) = 0.8, which means that 80% of smokers use the plan. Then:\n",
    "\n",
    "P(B|A) = 0.4 P(A) = 0.7 P(B) = 0.34 P(A|B) = P(B|A) * P(A) / P(B) = 0.4 * 0.7 / 0.34 = 0.82\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.82."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786f48c6-7c5c-4f30-8353-a34c663cee8e",
   "metadata": {},
   "source": [
    "# Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3633d926-7875-4167-8811-c1a44ff0324c",
   "metadata": {},
   "source": [
    "Ans: Bernoulli Naive Bayes and Multinomial Naive Bayes are both variants of the Naive Bayes algorithm used for classification tasks, but they differ in the type of input data they are designed to handle.\n",
    "\n",
    "Bernoulli Naive Bayes is used for binary data where each feature can only take one of two values, typically 0 or 1. It assumes that each feature is conditionally independent of all other features given the class label. For example, in text classification, each feature can represent the presence or absence of a specific word in the document.\n",
    "\n",
    "Multinomial Naive Bayes, on the other hand, is used for discrete count data where each feature represents the frequency of a particular event. It assumes that the features are conditionally independent of each other given the class label, and it can handle multiple occurrences of the same feature in a document. For example, in text classification, each feature can represent the count of a specific word in the document.\n",
    "\n",
    "To summarize, Bernoulli Naive Bayes is appropriate when dealing with binary data, while Multinomial Naive Bayes is designed to handle count data with multiple occurrences of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7168c6f-f881-449f-933b-e818a9b3f88e",
   "metadata": {},
   "source": [
    "# Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8383cfeb-98ed-4270-9f89-9db17a01c1a1",
   "metadata": {},
   "source": [
    "Ans: Bernoulli Naive Bayes assumes that each feature is binary, taking the value 0 or 1. Therefore, it cannot handle missing values in the traditional sense, since a missing value cannot be interpreted as either 0 or 1.\n",
    "\n",
    "One common approach to dealing with missing values in Bernoulli Naive Bayes is to treat them as a separate category, distinct from 0 and 1. This approach is known as the \"missing category\" or \"unknown category\" approach. In this approach, a missing value is treated as a third category, and the model learns separate probabilities for each category.\n",
    "\n",
    "Another approach is to impute missing values by assigning a value based on the distribution of the observed values. For example, if a feature is 1 in 60% of the cases where it is observed and 0 in the remaining 40%, a missing value could be imputed as a 1 with 60% probability and as a 0 with 40% probability.\n",
    "\n",
    "It's worth noting that the choice of how to handle missing values can have a significant impact on the performance of the classifier. In some cases, it may be beneficial to remove samples or features with missing values rather than imputing them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d330072-0511-4617-b70a-e530e4e8b545",
   "metadata": {},
   "source": [
    "# Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a76d3b1-03fd-4adf-b611-ff2d4e35fbdc",
   "metadata": {},
   "source": [
    "Ans: Yes, Gaussian Naive Bayes can be used for multi-class classification, where the goal is to classify instances into more than two classes.\n",
    "\n",
    "One common approach for multi-class classification using Gaussian Naive Bayes is the one-vs-all (also known as one-vs-rest) strategy. In this approach, we train one binary classifier for each class, with the positive class being the one we are interested in and the negative class being all the other classes combined. During inference, we apply each classifier to the test instance and choose the class that has the highest posterior probability.\n",
    "\n",
    "Another approach is the pairwise approach, where we train a binary classifier for each pair of classes, and during inference, we classify a new instance by comparing the scores from all pairs of classifiers.\n",
    "\n",
    "It's important to note that while Gaussian Naive Bayes can be used for multi-class classification, it may not always be the best choice, especially when the classes are highly imbalanced or the features are not normally distributed. In such cases, other classifiers such as logistic regression or support vector machines may be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c408a8dc-2f27-4c84-a317-d1372d4687f6",
   "metadata": {},
   "source": [
    "# Q5. Assignment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36253d5c-ac5a-475d-b436-e14413722696",
   "metadata": {},
   "source": [
    "# Data preparation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74805cc5-0ba7-43b5-bbc2-2cc78bcd7ba1",
   "metadata": {},
   "source": [
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/ datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message is spam or not based on several input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79156bf-0bbc-4096-8ce1-607f13012781",
   "metadata": {},
   "source": [
    "# Implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6059461e-a592-47a9-9bb4-6ed1a432c640",
   "metadata": {},
   "source": [
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the dataset. You should use the default hyperparameters for each classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0833c1-1e9d-436c-9056-6f328f77dd2d",
   "metadata": {},
   "source": [
    "# Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f61e143-5628-4dcc-bd8c-e897da5f202a",
   "metadata": {},
   "source": [
    "Report the following performance metrics for each classifier: Accuracy Precision Recall F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45c52bf-70a1-435c-8667-601bc99a53a1",
   "metadata": {},
   "source": [
    "# Discussion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c616b814-d966-4f78-bce8-1d629dec1552",
   "metadata": {},
   "source": [
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is the case? Are there any limitations of Naive Bayes that you observed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6d8783-a707-4a72-9395-c43809c46f85",
   "metadata": {},
   "source": [
    "# Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1a93c5-f59c-4b38-b407-fa53a28b3b5d",
   "metadata": {},
   "source": [
    "Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e91950-73a8-4bbf-a1a2-47655047c484",
   "metadata": {},
   "source": [
    "# Note: This dataset contains a binary classification problem with multiple features. The dataset is relatively small, but it can be used to demonstrate the performance of the different variants of Naive Bayes on a real-world problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c30cc21-d4ae-4115-876d-93ef05023790",
   "metadata": {},
   "source": [
    "Ans: I will now implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python. Then, I will use 10-fold cross-validation to evaluate the performance of each classifier on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cdd94c6-bdcb-4029-83ef-b4029aed800d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes:\n",
      "Accuracy: 0.88\n",
      "Precision: 0.89\n",
      "Recall: 0.82\n",
      "F1 Score: 0.85\n",
      "\n",
      "Multinomial Naive Bayes:\n",
      "Accuracy: 0.79\n",
      "Precision: 0.74\n",
      "Recall: 0.72\n",
      "F1 Score: 0.73\n",
      "\n",
      "Gaussian Naive Bayes:\n",
      "Accuracy: 0.82\n",
      "Precision: 0.71\n",
      "Recall: 0.96\n",
      "F1 Score: 0.81\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data', header=None)\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "# Bernoulli Naive Bayes classifier\n",
    "bnb = BernoulliNB()\n",
    "bnb_scores = cross_val_score(bnb, X, y, cv=10)\n",
    "\n",
    "# Multinomial Naive Bayes classifier\n",
    "mnb = MultinomialNB()\n",
    "mnb_scores = cross_val_score(mnb, X, y, cv=10)\n",
    "\n",
    "# Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "gnb_scores = cross_val_score(gnb, X, y, cv=10)\n",
    "\n",
    "# Print the results\n",
    "print(\"Bernoulli Naive Bayes:\")\n",
    "print(\"Accuracy: {:.2f}\".format(bnb_scores.mean()))\n",
    "print(\"Precision: {:.2f}\".format(cross_val_score(bnb, X, y, cv=10, scoring='precision').mean()))\n",
    "print(\"Recall: {:.2f}\".format(cross_val_score(bnb, X, y, cv=10, scoring='recall').mean()))\n",
    "print(\"F1 Score: {:.2f}\".format(cross_val_score(bnb, X, y, cv=10, scoring='f1').mean()))\n",
    "\n",
    "print(\"\\nMultinomial Naive Bayes:\")\n",
    "print(\"Accuracy: {:.2f}\".format(mnb_scores.mean()))\n",
    "print(\"Precision: {:.2f}\".format(cross_val_score(mnb, X, y, cv=10, scoring='precision').mean()))\n",
    "print(\"Recall: {:.2f}\".format(cross_val_score(mnb, X, y, cv=10, scoring='recall').mean()))\n",
    "print(\"F1 Score: {:.2f}\".format(cross_val_score(mnb, X, y, cv=10, scoring='f1').mean()))\n",
    "\n",
    "print(\"\\nGaussian Naive Bayes:\")\n",
    "print(\"Accuracy: {:.2f}\".format(gnb_scores.mean()))\n",
    "print(\"Precision: {:.2f}\".format(cross_val_score(gnb, X, y, cv=10, scoring='precision').mean()))\n",
    "print(\"Recall: {:.2f}\".format(cross_val_score(gnb, X, y, cv=10, scoring='recall').mean()))\n",
    "print(\"F1 Score: {:.2f}\".format(cross_val_score(gnb, X, y, cv=10, scoring='f1').mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de693c9-3fcc-4164-b069-d768e4b5940e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
