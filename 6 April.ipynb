{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84d9f38d-703b-4f23-822b-f60522428cad",
   "metadata": {},
   "source": [
    "# Q1. What is the mathematical formula for a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ac58e8-70d3-47c8-a8ea-74cdb44dc5b6",
   "metadata": {},
   "source": [
    "Ans: A linear SVM (Support Vector Machine) is a type of machine learning algorithm used for classification tasks. It finds a linear boundary (also called hyperplane) that separates data points of different classes in a high-dimensional space. The mathematical formula for a linear SVM can be expressed as follows:\n",
    "\n",
    "Given a training set of input vectors x and corresponding binary output labels y (either -1 or +1), the goal of a linear SVM is to find a weight vector w and bias term b such that the decision boundary is defined as:\n",
    "\n",
    "f(x) = sign(w*x + b)\n",
    "\n",
    "where sign() is the sign function which returns -1 or +1 depending on whether the argument is negative or positive.\n",
    "\n",
    "The weight vector w and bias term b are determined by solving the following optimization problem:\n",
    "\n",
    "minimize 0.5||w||^2 subject to y_i(w*x_i + b) >= 1 for all i\n",
    "\n",
    "where ||w|| is the L2 norm of the weight vector and i indexes the training examples. The optimization problem seeks to find the maximum-margin hyperplane that separates the two classes of data points, while also satisfying the constraint that all points are correctly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617ff26d-6966-49ce-bf5c-d015f5d7d8ff",
   "metadata": {},
   "source": [
    "# Q2. What is the objective function of a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f33495-2867-459a-87eb-a524cf7d446f",
   "metadata": {},
   "source": [
    "Ans: The objective function of a linear Support Vector Machine (SVM) is to find the hyperplane that maximally separates the training data into two classes.\n",
    "\n",
    "In other words, the objective of a linear SVM is to find the optimal hyperplane that maximizes the margin, which is the distance between the hyperplane and the closest points from each class (called support vectors). This hyperplane will correctly classify the training data, and it is expected to generalize well to unseen data.\n",
    "\n",
    "Mathematically, the objective function of a linear SVM can be expressed as:\n",
    "\n",
    "minimize 1/2 * ||w||^2\n",
    "\n",
    "subject to y_i(w^T*x_i+b) >= 1 for all i=1,...,n,\n",
    "\n",
    "where w is the weight vector, b is the bias term, x_i is the i-th training example, y_i is its corresponding label (either -1 or 1), and n is the number of training examples.\n",
    "\n",
    "The first term in the objective function represents the regularization term, which penalizes large weight values, and the second term represents the margin constraints. The optimization problem aims to minimize the regularization term while satisfying the margin constraints for all training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526dab32-08d8-4a20-8ec6-54633ad851ba",
   "metadata": {},
   "source": [
    "# Q3. What is the kernel trick in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17f794b-590e-4542-96a5-352e4f4cd6b6",
   "metadata": {},
   "source": [
    "Ans: The kernel trick is a technique used in Support Vector Machines (SVMs) to transform the input data into a higher-dimensional space without explicitly computing the transformed features. The idea behind this technique is to use a kernel function that computes the inner product of the transformed data points without actually computing the transformed data points themselves.\n",
    "\n",
    "In other words, the kernel function takes the original data points as inputs and returns the inner product of their transformed versions, which can be used as a measure of similarity between the data points in the higher-dimensional space.\n",
    "\n",
    "The use of the kernel function makes it possible to apply SVMs to non-linearly separable data by mapping the data points to a higher-dimensional space, where they are more likely to be linearly separable.\n",
    "\n",
    "The most commonly used kernel functions in SVMs are the linear kernel, polynomial kernel, and radial basis function (RBF) kernel. The choice of kernel function depends on the nature of the data and the problem at hand.\n",
    "\n",
    "The use of the kernel trick in SVMs has the advantage of allowing the algorithm to learn complex decision boundaries in high-dimensional spaces, without explicitly computing the transformed features. This can result in better performance and more efficient computation, compared to explicitly computing the transformed features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c37e4-225f-4b26-babe-e2ba31c60f3b",
   "metadata": {},
   "source": [
    "# Q4. What is the role of support vectors in SVM Explain with example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccfd536-86d5-4ba4-8cc5-66faaf83d2de",
   "metadata": {},
   "source": [
    "Ans: n SVM, support vectors play a critical role in determining the position of the decision boundary, which separates the different classes of data points.\n",
    "\n",
    "Support vectors are the data points that lie closest to the decision boundary and have a non-zero weight in the SVM model. They are the most influential data points in defining the position and orientation of the decision boundary.\n",
    "\n",
    "For example, consider a binary classification problem where the goal is to separate two classes of data points, represented by red and blue dots in a two-dimensional space. The SVM algorithm will find the hyperplane that best separates the two classes of data points, as shown in the figure below: In this example, the support vectors are the data points that lie closest to the decision boundary, as shown by the black circles in the figure. These support vectors play a critical role in defining the position and orientation of the decision boundary, and any changes to the position or orientation of the boundary will only affect the classification of data points that are closer to the boundary than the support vectors.\n",
    "\n",
    "The SVM algorithm aims to maximize the margin, which is the distance between the decision boundary and the closest support vectors. By maximizing the margin, the algorithm tries to ensure that the decision boundary is as far away from the closest data points as possible, which can help improve the generalization performance of the SVM model.\n",
    "\n",
    "The support vectors are also used to classify new data points. A new data point is classified based on its distance to the decision boundary and the position of the support vectors. If a new data point is closer to one set of support vectors than the other, it will be classified as belonging to the class associated with those support vectors.\n",
    "\n",
    "Overall, the support vectors are essential in SVM as they determine the position of the decision boundary, play a critical role in the classification of new data points, and help improve the generalization performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dc94e1-341e-44b1-ad8c-ec67d55d3835",
   "metadata": {},
   "source": [
    "# Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf25ad48-db87-45cc-9231-7df77c751302",
   "metadata": {},
   "source": [
    "Ans: Sure, let me explain each of these concepts with examples and graphs.\n",
    "\n",
    "1. Hyperplane:\n",
    "\n",
    "The hyperplane is the decision boundary that separates the two classes in SVM. In a binary classification problem, the hyperplane is a line in 2D, a plane in 3D, and a hyperplane in higher dimensions. The goal of SVM is to find the hyperplane that maximizes the margin between the two classes.\n",
    "\n",
    "2. Marginal plane:\n",
    "\n",
    "The marginal plane is the plane that runs parallel to the hyperplane and touches the support vectors. The distance between the hyperplane and the marginal plane is called the margin. In SVM, the goal is to maximize the margin between the hyperplane and the marginal plane.\n",
    "\n",
    "3. Hard margin:\n",
    "\n",
    "In hard margin SVM, the algorithm tries to find a hyperplane that perfectly separates the two classes of data points without any errors. This works only if the data points are linearly separable.\n",
    "\n",
    "4. Soft margin:\n",
    "\n",
    "In soft margin SVM, the algorithm allows some misclassification errors by introducing a slack variable that relaxes the strictness of the margin. This helps in cases where the data points are not linearly separable. The objective function of soft margin SVM is to find a hyperplane that minimizes the errors and maximizes the margin.\n",
    "\n",
    "Overall, these concepts are fundamental to understanding SVM and its various types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587d514b-2f18-43b1-92b1-00b5e0c6fe4b",
   "metadata": {},
   "source": [
    "# Q6. SVM Implementation through Iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dcfd96-12af-4dfd-8f42-2bd570c07ee8",
   "metadata": {},
   "source": [
    "# Bonus task: Implement a linear SVM classifier from scratch using Python and compare its "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceddbfd-d473-4a4b-94a5-1c1838d98c2d",
   "metadata": {},
   "source": [
    "performance with the scikit-learn implementation.\n",
    "\n",
    "1. Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "2. Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "3. Compute the accuracy of the model on the testing setl\n",
    "4. Plot the decision boundaries of the trained model using two of the featuresl\n",
    "5. Try different values of the regularisation parameter C and see how it affects the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4df5f7b2-4a77-4fa4-897c-ca780d12232d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d5dda10-6965-48f7-a8e3-60b94bc16ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris=load_iris()\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(iris.data,iris.target,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a97690d0-1053-45bc-8552-1afbaff5eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc=SVC(kernel='linear')\n",
    "svc.fit(X_train,y_train)\n",
    "y_pred=svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84fe8301-0285-4079-b88a-e3d55da2ec6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0 13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "849906a3-b6a6-46de-b256-4e0f94b064f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_decision_regions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m svm_2feat \u001b[38;5;241m=\u001b[39m SVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, C\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      9\u001b[0m svm_2feat\u001b[38;5;241m.\u001b[39mfit(X_train_2feat, y_train)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mplot_decision_regions\u001b[49m(X_train_2feat, y_train, clf\u001b[38;5;241m=\u001b[39msvm_2feat)\n\u001b[1;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpetal length [cm]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpetal width [cm]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_decision_regions' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "# Plot decision regions for two features\n",
    "X_train_2feat = X_train[:, [2, 3]]\n",
    "X_test_2feat = X_test[:, [2, 3]]\n",
    "svm_2feat = SVC(kernel='linear', C=1)\n",
    "svm_2feat.fit(X_train_2feat, y_train)\n",
    "plot_decision_regions(X_train_2feat, y_train, clf=svm_2feat)\n",
    "plt.xlabel('petal length [cm]')\n",
    "plt.ylabel('petal width [cm]')\n",
    "plt.title('SVM Decision Region Boundary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8fbd860-0e35-4472-8a96-f492e343b831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C =  0.1 Accuracy: 1.0\n",
      "C =  1 Accuracy: 1.0\n",
      "C =  10 Accuracy: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "# Try different values of the regularization parameter C\n",
    "C_values = [0.1, 1, 10]\n",
    "for C in C_values:\n",
    "    svm = SVC(kernel='linear', C=C)\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_pred = svm.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"C = \", C, \"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27721364-3e00-47ba-8373-dde3625091ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
