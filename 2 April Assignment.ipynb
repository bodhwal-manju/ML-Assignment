{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3717d0cb-ba30-49fd-8d45-5b67e6745c53",
   "metadata": {},
   "source": [
    "# Q1. What is the purpose of grid search cv in machine learning, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b21df4a-5a86-4acb-a584-d2a063ed9a6b",
   "metadata": {},
   "source": [
    "Ans: The purpose of grid search CV (cross-validation) in machine learning is to find the optimal hyperparameters for a given model. Hyperparameters are parameters that are not learned by the model during training, but rather are set prior to training, such as the learning rate, regularization parameter, or number of hidden layers. Grid search CV automates the process of hyperparameter tuning by searching through a predefined grid of hyperparameter values and evaluating the model's performance using cross-validation.\n",
    "\n",
    "The grid search CV process works as follows:\n",
    "\n",
    "1. Define the range of hyperparameters to search over: This involves specifying a range of hyperparameter values to search over, such as a range of learning rates or regularization strengths.\n",
    "\n",
    "2. Define the evaluation metric: This involves selecting an appropriate evaluation metric to use for comparing different hyperparameters, such as accuracy, precision, recall, F1 score, or AUC.\n",
    "\n",
    "3. Define the cross-validation scheme: This involves selecting an appropriate cross-validation scheme to use for evaluating the model's performance, such as k-fold cross-validation or stratified k-fold cross-validation.\n",
    "\n",
    "4. Generate combinations of hyperparameters: This involves generating all possible combinations of hyperparameters based on the specified ranges.\n",
    "\n",
    "5. Train and evaluate the model: For each combination of hyperparameters, train the model using the training set and evaluate its performance using the evaluation  metric on the validation set.\n",
    "\n",
    "6. Select the best hyperparameters: After evaluating all combinations of hyperparameters, select the combination that gives the best performance on the evaluation metric.\n",
    "\n",
    "7. Retrain the model on the full dataset: Once the best hyperparameters have been selected, retrain the model on the full dataset using these hyperparameters.\n",
    "\n",
    "Grid search CV can be computationally expensive, especially when there are many hyperparameters to search over or when the dataset is large. However, it can be a powerful tool for automating the process of hyperparameter tuning and improving the performance of machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1032078c-8b33-49df-9f4f-f9962140369f",
   "metadata": {},
   "source": [
    "# Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9754299c-46fd-400f-a0e5-d82dad46074b",
   "metadata": {},
   "source": [
    "Ans: Grid search CV and randomized search CV are both methods for hyperparameter tuning, but they differ in the way they explore the hyperparameter space.\n",
    "\n",
    "Grid search CV involves searching over a predefined grid of hyperparameter values, where each combination of hyperparameters is evaluated using cross-validation. This method is useful when the hyperparameter space is relatively small and the number of hyperparameters is not too high, as it exhaustively explores all combinations of hyperparameters. However, it can become computationally expensive when the hyperparameter space is large, leading to long training times and a higher risk of overfitting if the number of hyperparameters is too high.\n",
    "\n",
    "Randomized search CV, on the other hand, involves searching over a randomized distribution of hyperparameter values, where each combination of hyperparameters is evaluated using cross-validation. This method is useful when the hyperparameter space is large, as it can explore a wide range of hyperparameters without evaluating every possible combination. By randomly sampling hyperparameters from a distribution, randomized search CV can find good hyperparameters more efficiently than grid search CV, with less computational cost and less risk of overfitting.\n",
    "\n",
    "In general, grid search CV is more suitable when the number of hyperparameters is relatively small, and we have some prior knowledge of the possible values for each hyperparameter. Randomized search CV is more suitable when the number of hyperparameters is high and we have less prior knowledge of the possible values for each hyperparameter. Randomized search CV is also useful when computational resources are limited, and we want to find a good set of hyperparameters efficiently.\n",
    "\n",
    "Ultimately, the choice between grid search CV and randomized search CV depends on the specific problem and the hyperparameter space being explored. A good approach is to try both methods and compare their performance in terms of evaluation metric and computational cost to determine which method is more suitable for the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec457a78-5d19-499c-8b54-ec0b9458c7b1",
   "metadata": {},
   "source": [
    "# Q3. What is data leakage, and why is it a problem in machine learning? Provide an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c6900f-ad1a-4d89-89fa-89f6d85d009c",
   "metadata": {},
   "source": [
    "Ans: Data leakage refers to a situation where information that is not supposed to be available to the model during training is inadvertently included in the training data. This can lead to a biased and overly optimistic evaluation of the model's performance, as the model may be learning patterns in the data that it would not have access to in practice.\n",
    "\n",
    "Data leakage can occur in many ways, including:\n",
    "\n",
    "1. Including target variables that are only available in the test set, such as in time series data where future values are not available during training.\n",
    "\n",
    "2. Including variables that are derived from the target variable, such as including the average of the target variable for each group in the training data, which can provide the model with information about the target variable that it would not have access to in practice.\n",
    "\n",
    "3. Including variables that are correlated with the target variable, such as including the exact time of day in the training data for a model predicting daily sales, which can provide the model with information about the target variable that it would not have access to in practice.\n",
    "\n",
    "4. Using cross-validation incorrectly, such as performing feature selection or preprocessing steps before splitting the data into train and test sets, which can leak information about the test set into the training data.\n",
    "\n",
    "Data leakage is a problem in machine learning because it can lead to overfitting, where the model performs well on the training data but poorly on the test data. This can result in models that are not generalizable to new data, and can lead to incorrect conclusions and decisions based on the model's predictions.\n",
    "\n",
    "An example of data leakage would be building a model to predict credit risk, where the training data includes the credit scores of the borrowers. Credit scores are often only available after the loan is issued, so including this information in the training data would be data leakage. The model would learn to use the credit scores to predict credit risk, which would not be possible in practice, leading to overfitting and poor generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa2e0dd-e50e-405e-9e30-7549ea54056e",
   "metadata": {},
   "source": [
    "# Q4. How can you prevent data leakage when building a machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1652ed-a4f8-4ca1-af24-fc342142505b",
   "metadata": {},
   "source": [
    "Ans: Preventing data leakage requires careful consideration of the data and how it is processed during the machine learning pipeline. Some best practices to prevent data leakage include:\n",
    "\n",
    "1. Ensuring that the training and test data are properly separated before any data preprocessing or feature engineering is performed.\n",
    "\n",
    "2. Avoiding the use of features that are only available in the test set, such as future data points in time series forecasting or external data that is only available after the training data period.\n",
    "\n",
    "3. Ensuring that any feature engineering or data preprocessing steps are applied separately to the training and test sets, so that the test set remains independent of the training set.\n",
    "\n",
    "4. Using cross-validation techniques that properly separate the training and validation sets and prevent data leakage. One such technique is to perform all feature engineering and preprocessing steps within each fold of the cross-validation, so that the validation set remains independent of the training set.\n",
    "\n",
    "5. Being aware of any biases in the data and avoiding features that may lead to data leakage, such as features that are highly correlated with the target variable or that provide information that is not available in practice.\n",
    "\n",
    "By following these best practices and carefully considering the data and how it is processed, we can prevent data leakage and ensure that our machine learning models are properly trained and evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89c9291-d96e-4ca1-baad-341e08bc63c8",
   "metadata": {},
   "source": [
    "# Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a90abd7-a079-4a7b-ad69-1a926d75ae1a",
   "metadata": {},
   "source": [
    "Ans: A confusion matrix is a table that is used to evaluate the performance of a classification model. It displays the predicted and actual class labels for a set of data, allowing us to see how well the model is performing.\n",
    "\n",
    "A confusion matrix is typically organized into a 2x2 table, with the actual class labels along one axis and the predicted class labels along the other axis. The four possible outcomes are:\n",
    "\n",
    "1. True positive (TP): the model predicted the positive class correctly.\n",
    "2. False positive (FP): the model predicted the positive class incorrectly.\n",
    "3. True negative (TN): the model predicted the negative class correctly.\n",
    "4. False negative (FN): the model predicted the negative class incorrectly.\n",
    "\n",
    "Using these outcomes, we can calculate several performance metrics, including:\n",
    "\n",
    "1. Accuracy: the proportion of correct predictions, calculated as (TP+TN)/(TP+FP+TN+FN).\n",
    "2. Precision: the proportion of true positive predictions out of all positive predictions, calculated as TP/(TP+FP).\n",
    "3. Recall (also called sensitivity or true positive rate): the proportion of true positive predictions out of all actual positive cases, calculated as TP/(TP+FN).\n",
    "4. pecificity (also called true negative rate): the proportion of true negative predictions out of all actual negative cases, calculated as TN/(TN+FP).\n",
    "5. F1-score: a harmonic mean of precision and recall, calculated as 2(precisionrecall)/(precision+recall).\n",
    "\n",
    "These metrics provide insight into how well the model is performing for different classes, and can help identify areas for improvement. For example, a high recall and low precision may indicate that the model is correctly identifying most positive cases, but is also misclassifying many negative cases as positive. A high specificity and low sensitivity may indicate that the model is correctly identifying negative cases, but is missing many positive cases. By examining the confusion matrix and calculating performance metrics, we can gain a better understanding of how our classification model is performing and make adjustments to improve its accuracy and reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce2aeee-d4ed-4dfd-b805-9ffd349db236",
   "metadata": {},
   "source": [
    "# Q6. Explain the difference between precision and recall in the context of a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1289f8b-c145-4fb2-a5e9-2c99dbf3eb44",
   "metadata": {},
   "source": [
    "Ans: In the context of a confusion matrix, precision and recall are two important metrics that are used to evaluate the performance of a classification model.\n",
    "\n",
    "Precision is the proportion of true positive predictions out of all positive predictions made by the model. In other words, it measures the accuracy of positive predictions. Mathematically, precision is defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b8a2445-1e3b-4431-9647-6e9f91a591ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'precision = true positives / (true positives + false positives'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"precision = true positives / (true positives + false positives\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdf810f-9072-4315-bdc5-63e3746ac726",
   "metadata": {},
   "source": [
    "Recall, on the other hand, is the proportion of true positive predictions out of all actual positive cases in the dataset. In other words, it measures the completeness of positive predictions. Mathematically, recall is defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b51b618-78e3-4435-a7e5-8f7d797f9ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'recall = true positives / (true positives + false negatives)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"recall = true positives / (true positives + false negatives)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999f072c-3c61-4b2c-a6e8-848147f248a6",
   "metadata": {},
   "source": [
    "To understand the difference between precision and recall, consider the following example. Suppose we have a binary classification model that predicts whether a patient has a certain disease or not. In this case, the positive class refers to patients who have the disease, and the negative class refers to patients who do not have the disease. A confusion matrix for this scenario may look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "550e953a-fdf7-41e3-91e6-1277594c2c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              Predicted\\n              Negative  Positive\\nActual  Negative    TN        FP\\n      Positive    FN        TP'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  \"\"\"              Predicted\n",
    "                Negative  Positive\n",
    "Actual  Negative    TN        FP\n",
    "        Positive    FN        TP\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d1ec1-8795-43a2-a421-8588cbc6e09f",
   "metadata": {},
   "source": [
    "In this confusion matrix, true positives (TP) are patients who have the disease and are correctly identified as having the disease by the model. False positives (FP) are patients who do not have the disease but are incorrectly identified as having the disease by the model. False negatives (FN) are patients who have the disease but are incorrectly identified as not having the disease by the model. True negatives (TN) are patients who do not have the disease and are correctly identified as not having the disease by the model.\n",
    "\n",
    "Precision and recall can be interpreted in the following ways:\n",
    "\n",
    "1. Precision: Of all patients predicted to have the disease, what proportion actually have the disease?\n",
    "2. Recall: Of all patients who actually have the disease, what proportion are correctly identified by the model as having the disease?\n",
    "\n",
    "In general, precision and recall are inversely related; improving one typically comes at the expense of the other. A high precision indicates that the model is correctly identifying positive cases, but it may be missing some true positives. A high recall indicates that the model is correctly identifying most of the positive cases, but it may also be misclassifying some negative cases as positive. The balance between precision and recall depends on the specific problem and the costs associated with false positives and false negatives, and it is important to consider both metrics when evaluating a classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe675a-afd4-4740-ba27-c227ca6ec7fc",
   "metadata": {},
   "source": [
    "# Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84ee3ef-2ac0-465c-bb34-893af15bc9f2",
   "metadata": {},
   "source": [
    "Ans: A confusion matrix is a table that summarizes the performance of a classification model by comparing the predicted labels to the true labels. It provides a detailed breakdown of the model's correct and incorrect predictions, organized by true and predicted labels. The matrix typically has four entries: true positives (TP), false positives (FP), false negatives (FN), and true negatives (TN).\n",
    "\n",
    "To interpret a confusion matrix and determine which types of errors your model is making, you can focus on the following metrics:\n",
    "\n",
    "1. Accuracy: This metric measures the overall performance of the model by calculating the proportion of correct predictions. It is calculated as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "729e3582-0c2e-4e2d-b1a0-9a4bb9ba9107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accuracy = (TP + TN) / (TP + TN + FP + FN)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"accuracy = (TP + TN) / (TP + TN + FP + FN)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ff50fb-e63d-4cdd-aa5b-f9b48ac52949",
   "metadata": {},
   "source": [
    "Accuracy is a good metric to use when the classes are balanced, but it can be misleading when the classes are imbalanced.\n",
    "\n",
    "2. Precision: This metric measures the proportion of true positive predictions out of all positive predictions made by the model. It is calculated as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15f431f1-9c32-4a46-b843-72ef3169af05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'precision = TP / (TP + FP)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"precision = TP / (TP + FP)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5d1175-a65b-43a3-8d8a-4c1917bca310",
   "metadata": {},
   "source": [
    "Precision is a good metric to use when the cost of false positives is high.\n",
    "\n",
    "3. Recall: This metric measures the proportion of true positive predictions out of all actual positive cases in the dataset. It is calculated as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc96c401-7269-4e5b-8112-95c93dc5c1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'recall = TP / (TP + FN)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"recall = TP / (TP + FN)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a74981-7520-4162-a9e3-1f5295cacdd9",
   "metadata": {},
   "source": [
    "Recall is a good metric to use when the cost of false negatives is high.\n",
    "\n",
    "4. F1 score: This metric is the harmonic mean of precision and recall and provides a balanced evaluation of the model's performance. It is calculated as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3920ef2e-d09b-4506-9e55-00fc8b1a8b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F1 score = 2 * (precision * recall) / (precision + recall)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"F1 score = 2 * (precision * recall) / (precision + recall)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06b9107-2521-47dc-acb9-427ae5fb88ed",
   "metadata": {},
   "source": [
    "Once you have calculated these metrics, you can analyze the confusion matrix to determine which types of errors your model is making. For example:\n",
    "\n",
    "1. False positives (FP): The model predicts a positive label, but the true label is negative. This means that the model is incorrectly identifying negative cases as positive. False positives can be problematic when the cost of false positives is high, such as in medical diagnoses.\n",
    "\n",
    "2. False negatives (FN): The model predicts a negative label, but the true label is positive. This means that the model is incorrectly identifying positive cases as negative. False negatives can be problematic when the cost of false negatives is high, such as in fraud detection.\n",
    "\n",
    "3. True positives (TP): The model predicts a positive label, and the true label is also positive. This means that the model is correctly identifying positive cases as positive.\n",
    "\n",
    "4. True negatives (TN): The model predicts a negative label, and the true label is also negative. This means that the model is correctly identifying negative cases as negative.\n",
    "\n",
    "By analyzing the confusion matrix and these metrics, you can gain insights into which types of errors your model is making and make informed decisions on how to improve its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f36d88-4215-4e27-a1c8-3e3ec8004625",
   "metadata": {},
   "source": [
    "# Q8. What are some common metrics that can be derived from a confusion matrix, and how are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794a9826-7850-435c-8d2e-6c4d7f8f0f89",
   "metadata": {},
   "source": [
    "Ans: There are several common metrics that can be derived from a confusion matrix:\n",
    "\n",
    "1. Accuracy: This measures the proportion of correct predictions out of all predictions made by the model. It is calculated as:\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "2. Precision: This measures the proportion of true positive predictions out of all positive predictions made by the model. It is calculated as:\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "3. Recall (or Sensitivity or True Positive Rate): This measures the proportion of true positive predictions out of all actual positive cases in the dataset. It is calculated as:\n",
    "\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "4. Specificity (or True Negative Rate): This measures the proportion of true negative predictions out of all actual negative cases in the dataset. It is calculated as:\n",
    "\n",
    "Specificity = TN / (TN + FP)\n",
    "\n",
    "5. F1 Score: This is the harmonic mean of precision and recall and provides a balanced evaluation of the model's performance. It is calculated as:\n",
    "\n",
    "F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "6. False Positive Rate: This measures the proportion of false positive predictions out of all actual negative cases in the dataset. It is calculated as:\n",
    "\n",
    "False Positive Rate = FP / (TN + FP)\n",
    "\n",
    "7. False Negative Rate: This measures the proportion of false negative predictions out of all actual positive cases in the dataset. It is calculated as:\n",
    "\n",
    "False Negative Rate = FN / (TP + FN)\n",
    "\n",
    "These metrics can be used to evaluate the performance of a classification model and compare different models. It is important to select the appropriate metrics depending on the problem at hand and the cost of different types of errors. For example, in a medical diagnosis problem, the cost of a false negative (failing to diagnose a patient with a disease) may be higher than the cost of a false positive (diagnosing a patient with a disease when they do not have it), so recall may be a more important metric than precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c803b660-b3ed-4027-974e-cc0c62d20f84",
   "metadata": {},
   "source": [
    "# Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfea7ef-4718-40ab-a022-4344c0be6979",
   "metadata": {},
   "source": [
    "Ans: The accuracy of a model is calculated using the values in its confusion matrix. Specifically, accuracy is the proportion of correct predictions out of all predictions made by the model. It is calculated as:\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "where TP is the number of true positives, TN is the number of true negatives, FP is the number of false positives, and FN is the number of false negatives.\n",
    "\n",
    "The confusion matrix provides more detailed information about the model's performance beyond accuracy. It breaks down the number of true positives, false positives, true negatives, and false negatives, allowing for a more granular analysis of the model's strengths and weaknesses. For example, a model may have a high accuracy but a low recall, indicating that it is good at correctly predicting negative cases but may be missing positive cases.\n",
    "\n",
    "Overall, the confusion matrix provides a more nuanced evaluation of a model's performance compared to accuracy alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502d0752-03e0-43d3-9899-0a56c4688d89",
   "metadata": {},
   "source": [
    "# Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96df433-f18f-4107-939b-fad64ad2492d",
   "metadata": {},
   "source": [
    "Ans: A confusion matrix can help identify potential biases or limitations in a machine learning model by revealing patterns in the types of errors it makes.\n",
    "\n",
    "For example, if a model consistently misclassifies one particular class, this may indicate that the model is biased against that class. Alternatively, if a model has a high false positive rate, it may be overly aggressive in making positive predictions and need to be adjusted to be more conservative.\n",
    "\n",
    "Additionally, a confusion matrix can help identify imbalanced classes in the dataset. For example, if the number of instances in one class is much larger than the others, the model may be biased towards that class and perform poorly on the other classes. In such cases, adjusting the class weights or resampling techniques may help improve the model's performance.\n",
    "\n",
    "Overall, examining the confusion matrix can help provide insights into potential biases or limitations in a machine learning model and guide efforts to improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ad68a2-6ad5-4286-a2ab-beec012459b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
