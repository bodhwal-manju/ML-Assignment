{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbcd664c-a9b5-4f22-9379-bb9f64317ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max scaling, also known as normalization, is a common data preprocessing technique used to rescale numerical features within a specific range. It transforms the original data by subtracting the minimum value and dividing by the difference between the maximum and minimum values.\n",
      "     The formula for Min-Max scaling is as follows:\n",
      "\n",
      "scaled_value=(original value-min_value)/max_value -min_value\n",
      "example of its application:\n",
      "Area   | Bedrooms |\n",
      "|----------|----------|\n",
      "|   1500   |    2     |\n",
      "|   2000   |    3     |\n",
      "|   1000   |    1     |\n",
      "|   2500   |    4     |\n",
      "|    500   |    1     |\n",
      "\n",
      "To apply Min-Max scaling, we calculate the minimum and maximum values for each feature. In this case:\n",
      "\n",
      "Min \"area\" = 500, Max \"area\" = 3000\n",
      "Min \"bedrooms\" = 1, Max \"bedrooms\" = 5\n",
      "We use these values to scale the data using the formula mentioned earlier. After scaling, the dataset becomes:\n",
      "|   Area   | Bedrooms |\n",
      "|----------|----------|\n",
      "|   0.375  |   0.25   |\n",
      "|   0.5    |   0.5    |\n",
      "|   0.125  |   0.0    |\n",
      "|   0.75   |   0.75   |\n",
      "|   0.0    |   0.0    |\n",
      " Now both features are within the range of 0 to 1, allowing for fair comparison and analysis. Min-Max scaling ensures that no feature dominates the others due to their different original ranges, making it easier for machine learning models to learn and generalize patterns from the data.\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Q1\n",
    "print(\"\"\"Min-Max scaling, also known as normalization, is a common data preprocessing technique used to rescale numerical features within a specific range. It transforms the original data by subtracting the minimum value and dividing by the difference between the maximum and minimum values.\n",
    "     The formula for Min-Max scaling is as follows:\n",
    "\n",
    "scaled_value=(original value-min_value)/max_value -min_value\n",
    "example of its application:\n",
    "Area   | Bedrooms |\n",
    "|----------|----------|\n",
    "|   1500   |    2     |\n",
    "|   2000   |    3     |\n",
    "|   1000   |    1     |\n",
    "|   2500   |    4     |\n",
    "|    500   |    1     |\n",
    "\n",
    "To apply Min-Max scaling, we calculate the minimum and maximum values for each feature. In this case:\n",
    "\n",
    "Min \"area\" = 500, Max \"area\" = 3000\n",
    "Min \"bedrooms\" = 1, Max \"bedrooms\" = 5\n",
    "We use these values to scale the data using the formula mentioned earlier. After scaling, the dataset becomes:\n",
    "|   Area   | Bedrooms |\n",
    "|----------|----------|\n",
    "|   0.375  |   0.25   |\n",
    "|   0.5    |   0.5    |\n",
    "|   0.125  |   0.0    |\n",
    "|   0.75   |   0.75   |\n",
    "|   0.0    |   0.0    |\n",
    " Now both features are within the range of 0 to 1, allowing for fair comparison and analysis. Min-Max scaling ensures that no feature dominates the others due to their different original ranges, making it easier for machine learning models to learn and generalize patterns from the data.\n",
    "\n",
    " \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7d4aab2-4c88-476d-808b-be5d9c54e9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Unit Vector technique, also known as normalization or feature scaling by dividing by the Euclidean norm, is another data preprocessing method that rescales numerical features. Unlike Min-Max scaling, which brings the values within a specific range, the Unit Vector technique scales the features such that each data point becomes a vector with a Euclidean norm (length) of 1.\\n\\nLike Min-Max Scaling, the Unit Vector technique produces values of range [0,1]. The formula for Unit Vector scaling is as follows:\\n\\nscaled_value = original_value/sqrt(∑(original_value**2))\\n\\ndifference between unit vector technique and min max(normalisation) technique:\\nMin-Max scaling rescales the values to a specific range, while Unit Vector scaling normalizes the vectors to have a magnitude of 1. \\nMin-Max scaling focuses on the range of values, while Unit Vector scaling emphasizes the direction or orientation of the vectors.\\n\\nexample:\\nLet\\'s consider a dataset with three features: \"height,\" \"weight,\" and \"age.\" Each feature has different ranges:\\n\\n\"height\" ranges from 150 to 200 centimeters.\\n\"weight\" ranges from 50 to 100 kilograms.\\n\"age\" ranges from 20 to 60 years.\\nOriginal dataset:\\n| Height | Weight | Age |\\n|--------|--------|-----|\\n|  170   |   70   |  30 |\\n|  160   |   60   |  40 |\\n|  180   |   80   |  50 |\\n|  190   |   90   |  35 |\\nTo apply the Unit Vector technique, we calculate the Euclidean norm of each data point and divide each feature value by that norm. The dataset becomes:\\n\\nScaled dataset:\\n\\n|    Height    |    Weight    |     Age      |\\n|--------------|--------------|--------------|\\n|   0.654     |   0.262      |   0.131      |\\n|   0.707     |   0.353      |   0.471      |\\n|   0.707     |   0.353      |   0.471      |\\n|   0.663     |   0.297      |   0.298      |\\nIn the scaled dataset, each data point is represented as a vector with a Euclidean norm of 1. The feature values are adjusted to maintain the relative proportions between them, but the lengths of the vectors are now equal.\\n\\nThe Unit Vector technique emphasizes the directionality of the features and is particularly useful when working with algorithms that rely on cosine similarity or when the magnitudes of features are not directly comparable. It normalizes the features without limiting them to a specific range, which can be advantageous in certain contexts.\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"The Unit Vector technique, also known as normalization or feature scaling by dividing by the Euclidean norm, is another data preprocessing method that rescales numerical features. Unlike Min-Max scaling, which brings the values within a specific range, the Unit Vector technique scales the features such that each data point becomes a vector with a Euclidean norm (length) of 1.\n",
    "\n",
    "Like Min-Max Scaling, the Unit Vector technique produces values of range [0,1]. The formula for Unit Vector scaling is as follows:\n",
    "\n",
    "scaled_value = original_value/sqrt(∑(original_value**2))\n",
    "\n",
    "difference between unit vector technique and min max(normalisation) technique:\n",
    "Min-Max scaling rescales the values to a specific range, while Unit Vector scaling normalizes the vectors to have a magnitude of 1. \n",
    "Min-Max scaling focuses on the range of values, while Unit Vector scaling emphasizes the direction or orientation of the vectors.\n",
    "\n",
    "example:\n",
    "Let's consider a dataset with three features: \"height,\" \"weight,\" and \"age.\" Each feature has different ranges:\n",
    "\n",
    "\"height\" ranges from 150 to 200 centimeters.\n",
    "\"weight\" ranges from 50 to 100 kilograms.\n",
    "\"age\" ranges from 20 to 60 years.\n",
    "Original dataset:\n",
    "| Height | Weight | Age |\n",
    "|--------|--------|-----|\n",
    "|  170   |   70   |  30 |\n",
    "|  160   |   60   |  40 |\n",
    "|  180   |   80   |  50 |\n",
    "|  190   |   90   |  35 |\n",
    "To apply the Unit Vector technique, we calculate the Euclidean norm of each data point and divide each feature value by that norm. The dataset becomes:\n",
    "\n",
    "Scaled dataset:\n",
    "\n",
    "|    Height    |    Weight    |     Age      |\n",
    "|--------------|--------------|--------------|\n",
    "|   0.654     |   0.262      |   0.131      |\n",
    "|   0.707     |   0.353      |   0.471      |\n",
    "|   0.707     |   0.353      |   0.471      |\n",
    "|   0.663     |   0.297      |   0.298      |\n",
    "In the scaled dataset, each data point is represented as a vector with a Euclidean norm of 1. The feature values are adjusted to maintain the relative proportions between them, but the lengths of the vectors are now equal.\n",
    "\n",
    "The Unit Vector technique emphasizes the directionality of the features and is particularly useful when working with algorithms that rely on cosine similarity or when the magnitudes of features are not directly comparable. It normalizes the features without limiting them to a specific range, which can be advantageous in certain contexts.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d3e16ea-a6a5-4198-8039-ddbfba6b55aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Consider a dataset with three features: \"height,\" \"weight,\" and \"age.\" We want to reduce the dimensionality of the dataset while retaining the most important information.\\n\\nOriginal dataset:\\n| Height | Weight | Age |\\n|--------|--------|-----|\\n|  170   |   70   |  30 |\\n|  160   |   60   |  40 |\\n|  180   |   80   |  50 |\\n|  190   |   90   |  35 |\\nStandardize the data:\\nWe calculate the mean and standard deviation for each feature and standardize the dataset by subtracting the mean and dividing by the standard deviation.\\n\\nCompute the covariance matrix:\\nBased on the standardized dataset, we compute the covariance matrix to determine the relationships between the features.\\n\\nCalculate the eigenvectors and eigenvalues:\\nWe calculate the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the principal components, and the eigenvalues indicate their importance.\\n\\nSelect the number of principal components:\\nWe select the number of principal components based on the eigenvalues. Let\\'s say we choose to retain the first two principal components.\\n\\nProject the data onto the new feature space:\\nWe project the original data onto the selected principal components to obtain the lower-dimensional representation.\\nProjected dataset:\\n[[ 0.75333574 -1.11017353]\\n [ 1.86908426  0.36755333]\\n [-0.78887419  1.44636206]\\n [-1.83354581 -0.70374187]]\\nIn the projected dataset, the original three-dimensional data has been transformed into a two-dimensional space spanned by the first two principal components. This reduced-dimensional representation captures the most significant information in the data, allowing for easier visualization, analysis, or use in subsequent machine learning tasks.\\n\\nBy applying PCA, we can effectively reduce the dimensionality of the dataset while retaining the essential information and reducing computational complexity.\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q3\n",
    "\"\"\"PCA:PCA aims to find the directions of maximum variance in high-dimensional data and projects it onto a new subspace with equal or fewer dimensions than the original one.PCA (Principal Component Analysis) is a widely used technique in dimensionality reduction. It helps to transform a high-dimensional dataset into a lower-dimensional space while preserving the most important information or patterns in the data. It achieves this by identifying the principal components, which are new orthogonal axes that capture the maximum variance in the data.\n",
    "steps involved in PCA are as follows:\n",
    "1.Standardize the d-dimensional dataset.\n",
    "2.Construct the covariance matrix.\n",
    "3.Decompose the covariance matrix into its eigenvectors and eigenvalues.\n",
    "4.Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors.\n",
    "5.Select k eigenvectors which correspond to the k largest eigenvalues, where k is the dimensionality of the new feature subspace (k ≤ d).\n",
    "6.Construct a projection matrix W from the “top” k eigenvectors.\n",
    "7.Transform the d-dimensional input dataset X using the projection matrix W to obtain the new k-dimensional feature subspace.\n",
    "\"\"\"\n",
    "# Example:-\n",
    "\"\"\"Consider a dataset with three features: \"height,\" \"weight,\" and \"age.\" We want to reduce the dimensionality of the dataset while retaining the most important information.\n",
    "\n",
    "Original dataset:\n",
    "| Height | Weight | Age |\n",
    "|--------|--------|-----|\n",
    "|  170   |   70   |  30 |\n",
    "|  160   |   60   |  40 |\n",
    "|  180   |   80   |  50 |\n",
    "|  190   |   90   |  35 |\n",
    "Standardize the data:\n",
    "We calculate the mean and standard deviation for each feature and standardize the dataset by subtracting the mean and dividing by the standard deviation.\n",
    "\n",
    "Compute the covariance matrix:\n",
    "Based on the standardized dataset, we compute the covariance matrix to determine the relationships between the features.\n",
    "\n",
    "Calculate the eigenvectors and eigenvalues:\n",
    "We calculate the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the principal components, and the eigenvalues indicate their importance.\n",
    "\n",
    "Select the number of principal components:\n",
    "We select the number of principal components based on the eigenvalues. Let's say we choose to retain the first two principal components.\n",
    "\n",
    "Project the data onto the new feature space:\n",
    "We project the original data onto the selected principal components to obtain the lower-dimensional representation.\n",
    "Projected dataset:\n",
    "[[ 0.75333574 -1.11017353]\n",
    " [ 1.86908426  0.36755333]\n",
    " [-0.78887419  1.44636206]\n",
    " [-1.83354581 -0.70374187]]\n",
    "In the projected dataset, the original three-dimensional data has been transformed into a two-dimensional space spanned by the first two principal components. This reduced-dimensional representation captures the most significant information in the data, allowing for easier visualization, analysis, or use in subsequent machine learning tasks.\n",
    "\n",
    "By applying PCA, we can effectively reduce the dimensionality of the dataset while retaining the essential information and reducing computational complexity.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9282719e-6586-41dd-9e0e-a62388bc6fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projected dataset:\n",
      "[[ 0.75333574 -1.11017353]\n",
      " [ 1.86908426  0.36755333]\n",
      " [-0.78887419  1.44636206]\n",
      " [-1.83354581 -0.70374187]]\n"
     ]
    }
   ],
   "source": [
    "# example to illustrate  the application of PCA:\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Original dataset\n",
    "dataset = np.array([\n",
    "    [170, 70, 30],\n",
    "    [160, 60, 40],\n",
    "    [180, 80, 50],\n",
    "    [190, 90, 35]\n",
    "])\n",
    "\n",
    "# Standardize the dataset\n",
    "scaler = StandardScaler()\n",
    "scaled_dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)  # Select the number of components to retain\n",
    "reduced_dataset = pca.fit_transform(scaled_dataset)\n",
    "\n",
    "# Print the projected dataset\n",
    "print(\"Projected dataset:\")\n",
    "print(reduced_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6791cd17-caef-45aa-b7f5-b37e33e22fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Consider a dataset with five features: \"feature1,\" \"feature2,\" \"feature3,\" \"feature4,\" and \"feature5.\" We want to extract a reduced set of features that capture the most significant information in the data.\\n\\nOriginal dataset:\\n| Feature1 | Feature2 | Feature3 | Feature4 | Feature5 |\\n|----------|----------|----------|----------|----------|\\n|    5     |    2     |    3     |    4     |    1     |\\n|    7     |    1     |    4     |    3     |    2     |\\n|    3     |    4     |    6     |    2     |    5     |\\n|    1     |    6     |    5     |    1     |    4     |\\nBy using PCA fir Feature Extraction ,we follow the steps mentioned earlier\\n1.Standardize the data: We standardize the dataset by subtracting the mean and dividing by the standard deviation.\\n\\n2.Compute the covariance matrix: We calculate the covariance matrix to measure the relationships between the features.\\n\\n3.Calculate the eigenvectors and eigenvalues: We calculate the eigenvectors and eigenvalues of the covariance matrix.\\n\\n4.Select the number of principal components: Based on the eigenvalues, we determine the number of principal components to retain. Let\\'s say we choose to retain the first two principal components.\\n\\n5.Project the data onto the new feature space: We project the original data onto the selected principal components.\\n\\nProjected dataset:\\n|   PC1      |   PC2     |\\n| 0.75333574 |-1.11017353|\\n| 1.86908426 |0.36755333 |\\n|-0.78887419 |1.44636206 |\\n|-1.83354581 |-0.70374187|\\n\\n\\nIn the projected dataset, the original five-dimensional data has been transformed into a two-dimensional space spanned by the first two principal components. These principal components can be considered as the extracted features that capture the most important information in the data.\\nBy using PCA for feature extraction, we have reduced the dimensionality of the dataset while retaining the essential information. The extracted features can be further used for analysis, visualization, or as inputs to machine learning algorithms.\\nIt\\'s important to note that PCA as a feature extraction technique may not always provide easily interpretable features since they are linear combinations of the original features. However, it can be effective in capturing the most significant patterns or reducing the dimensionality of the data for further analysis or modeling purposes.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4\n",
    "\"\"\"PCA and feature extraction are closely related, as PCA can be used as a technique for feature extraction. Feature extraction aims to transform the original features into a new set of features that captures the essential information in the data. PCA, as a dimensionality reduction technique, achieves this by identifying the principal components that explain the maximum variance in the data. These principal components can be considered as the extracted features.\n",
    "\"\"\"\n",
    "# Example to illustrate how PCA can be used for feature Extraction\n",
    "\"\"\"Consider a dataset with five features: \"feature1,\" \"feature2,\" \"feature3,\" \"feature4,\" and \"feature5.\" We want to extract a reduced set of features that capture the most significant information in the data.\n",
    "\n",
    "Original dataset:\n",
    "| Feature1 | Feature2 | Feature3 | Feature4 | Feature5 |\n",
    "|----------|----------|----------|----------|----------|\n",
    "|    5     |    2     |    3     |    4     |    1     |\n",
    "|    7     |    1     |    4     |    3     |    2     |\n",
    "|    3     |    4     |    6     |    2     |    5     |\n",
    "|    1     |    6     |    5     |    1     |    4     |\n",
    "By using PCA fir Feature Extraction ,we follow the steps mentioned earlier\n",
    "1.Standardize the data: We standardize the dataset by subtracting the mean and dividing by the standard deviation.\n",
    "\n",
    "2.Compute the covariance matrix: We calculate the covariance matrix to measure the relationships between the features.\n",
    "\n",
    "3.Calculate the eigenvectors and eigenvalues: We calculate the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "4.Select the number of principal components: Based on the eigenvalues, we determine the number of principal components to retain. Let's say we choose to retain the first two principal components.\n",
    "\n",
    "5.Project the data onto the new feature space: We project the original data onto the selected principal components.\n",
    "\n",
    "Projected dataset:\n",
    "|   PC1      |   PC2     |\n",
    "| 0.75333574 |-1.11017353|\n",
    "| 1.86908426 |0.36755333 |\n",
    "|-0.78887419 |1.44636206 |\n",
    "|-1.83354581 |-0.70374187|\n",
    "\n",
    "\n",
    "In the projected dataset, the original five-dimensional data has been transformed into a two-dimensional space spanned by the first two principal components. These principal components can be considered as the extracted features that capture the most important information in the data.\n",
    "By using PCA for feature extraction, we have reduced the dimensionality of the dataset while retaining the essential information. The extracted features can be further used for analysis, visualization, or as inputs to machine learning algorithms.\n",
    "It's important to note that PCA as a feature extraction technique may not always provide easily interpretable features since they are linear combinations of the original features. However, it can be effective in capturing the most significant patterns or reducing the dimensionality of the data for further analysis or modeling purposes.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3dbc5ac-da69-4d30-920f-607758e57c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projected dataset:\n",
      "[[ 0.75333574 -1.11017353]\n",
      " [ 1.86908426  0.36755333]\n",
      " [-0.78887419  1.44636206]\n",
      " [-1.83354581 -0.70374187]]\n"
     ]
    }
   ],
   "source": [
    "#Example code\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ds=np.array([\n",
    "    [5, 2, 3, 4, 1],\n",
    "    [7, 1, 4, 3, 2],\n",
    "    [3, 4, 6, 2, 5],\n",
    "    [1, 6, 5, 1, 4]\n",
    "])\n",
    "\n",
    "# Standardize the dataset\n",
    "scaler=StandardScaler()\n",
    "scaled_dataset=scaler.fit_transform(ds)\n",
    "\n",
    "# perform PCA\n",
    "pca=PCA(n_components=2)\n",
    "reduced_ds=pca.fit_transform(scaled_dataset)\n",
    "print(\"Projected dataset:\")\n",
    "print(reduced_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ba435da-9f94-428b-9d6b-1ce1c34de702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To preprocess the dataset for building a recommendation system for a food delivery service, you can use Min-Max scaling on certain features such as price, rating, and delivery time. Here's how you can apply Min-Max scaling to preprocess the data:\\n\\n1.Identify the features: Determine which features in the dataset require scaling. In this case, the features that need scaling are price, rating, and delivery time.\\n\\n2.Calculate the minimum and maximum values: Calculate the minimum and maximum values for each feature across the dataset. For example, find the minimum and maximum prices, ratings, and delivery times.\\n\\n3.Apply Min-Max scaling: Use the Min-Max scaling formula to scale each feature individually. The formula is:\\nscaled_value = (original_value - min_value) / (max_value - min_value)\\n\\nApply this formula to each data point in the respective feature column, replacing the original values with the scaled values.\\n\\n4.Interpret the scaled values: After scaling, the values for each feature will be between 0 and 1. A value of 0 represents the minimum value in the original dataset, while a value of 1 represents the maximum value. The values in between are rescaled proportionally.\\n\\nBy applying Min-Max scaling, you ensure that all the features (price, rating, and delivery time) are transformed to the same scale, ranging from 0 to 1. This step is crucial for a recommendation system as it avoids any dominance or bias introduced by features with different scales. It allows the recommendation algorithm to consider all features equally when making recommendations, regardless of their original range or magnitude.\\n\\nOnce the data is preprocessed with Min-Max scaling, it can be used as input to the recommendation system, enabling it to analyze and compare the features accurately and provide relevant recommendations based on user preferences, such as price, rating, and delivery time.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5\n",
    "\"\"\"To preprocess the dataset for building a recommendation system for a food delivery service, you can use Min-Max scaling on certain features such as price, rating, and delivery time. Here's how you can apply Min-Max scaling to preprocess the data:\n",
    "\n",
    "1.Identify the features: Determine which features in the dataset require scaling. In this case, the features that need scaling are price, rating, and delivery time.\n",
    "\n",
    "2.Calculate the minimum and maximum values: Calculate the minimum and maximum values for each feature across the dataset. For example, find the minimum and maximum prices, ratings, and delivery times.\n",
    "\n",
    "3.Apply Min-Max scaling: Use the Min-Max scaling formula to scale each feature individually. The formula is:\n",
    "scaled_value = (original_value - min_value) / (max_value - min_value)\n",
    "\n",
    "Apply this formula to each data point in the respective feature column, replacing the original values with the scaled values.\n",
    "\n",
    "4.Interpret the scaled values: After scaling, the values for each feature will be between 0 and 1. A value of 0 represents the minimum value in the original dataset, while a value of 1 represents the maximum value. The values in between are rescaled proportionally.\n",
    "\n",
    "By applying Min-Max scaling, you ensure that all the features (price, rating, and delivery time) are transformed to the same scale, ranging from 0 to 1. This step is crucial for a recommendation system as it avoids any dominance or bias introduced by features with different scales. It allows the recommendation algorithm to consider all features equally when making recommendations, regardless of their original range or magnitude.\n",
    "\n",
    "Once the data is preprocessed with Min-Max scaling, it can be used as input to the recommendation system, enabling it to analyze and compare the features accurately and provide relevant recommendations based on user preferences, such as price, rating, and delivery time.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "689bc95f-0ce8-4fc5-bfd9-41aafcdab260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.04166667, 0.75      , 0.9       ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code:-\n",
    "import pandas as pd\n",
    "dataset=pd.DataFrame({\n",
    "    'price':[120,360,170,270,340],\n",
    "    'rating':[1,3,5,2,4],\n",
    "    'delivery time':[8,9,12,13,18],\n",
    "})\n",
    "dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max=MinMaxScaler()\n",
    "min_max.fit(dataset[['price','rating','delivery time']])\n",
    "min_max.transform(dataset[['price','rating','delivery time']])\n",
    "min_max.fit_transform(dataset[['price','rating','delivery time']])\n",
    "min_max.transform([[130,4,17]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e31ee2d-a51c-43d3-8a2c-872bedb8cd59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To reduce the dimensionality of the dataset containing many features for predicting stock prices, you can utilize PCA (Principal Component Analysis) as a dimensionality reduction technique. Here's how you can use PCA for this purpose:\\n\\n1.Prepare the dataset: Ensure that your dataset is properly prepared and cleaned, with relevant features extracted or engineered, and any missing values handled appropriately.\\n\\n2.Standardize the data: Since PCA is affected by the scale of the features, it is important to standardize the dataset to have zero mean and unit variance. This step ensures that features with larger variances do not dominate the analysis.\\n\\n3.Perform PCA: Apply PCA on the standardized dataset. PCA will identify the principal components (new feature vectors) that capture the most significant variance in the data.\\n\\n4.Determine the number of components: Analyze the explained variance ratio or eigenvalues associated with each principal component. The explained variance ratio shows the proportion of variance in the dataset explained by each principal component. Choose the number of principal components to retain based on the desired level of explained variance. For instance, you may choose to retain principal components that explain a cumulative variance of 90% or higher.\\n\\n5.Transform the data: Project the original dataset onto the selected principal components to obtain the reduced-dimensional representation of the data. This transformed dataset will have fewer dimensions, where each dimension represents a principal component.\\n\\nBy applying PCA, you can effectively reduce the dimensionality of the dataset while retaining the most important information or patterns present in the original data. This reduction in dimensions is beneficial for building models to predict stock prices as it helps to overcome the curse of dimensionality, reduces computational complexity, and can improve model performance by focusing on the most informative features.\\n\\nAfter reducing the dimensionality using PCA, you can use the transformed dataset as input to train your stock price prediction model, leveraging the reduced set of principal components instead of the original numerous features.\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#question 6\n",
    "\"\"\"To reduce the dimensionality of the dataset containing many features for predicting stock prices, you can utilize PCA (Principal Component Analysis) as a dimensionality reduction technique. Here's how you can use PCA for this purpose:\n",
    "\n",
    "1.Prepare the dataset: Ensure that your dataset is properly prepared and cleaned, with relevant features extracted or engineered, and any missing values handled appropriately.\n",
    "\n",
    "2.Standardize the data: Since PCA is affected by the scale of the features, it is important to standardize the dataset to have zero mean and unit variance. This step ensures that features with larger variances do not dominate the analysis.\n",
    "\n",
    "3.Perform PCA: Apply PCA on the standardized dataset. PCA will identify the principal components (new feature vectors) that capture the most significant variance in the data.\n",
    "\n",
    "4.Determine the number of components: Analyze the explained variance ratio or eigenvalues associated with each principal component. The explained variance ratio shows the proportion of variance in the dataset explained by each principal component. Choose the number of principal components to retain based on the desired level of explained variance. For instance, you may choose to retain principal components that explain a cumulative variance of 90% or higher.\n",
    "\n",
    "5.Transform the data: Project the original dataset onto the selected principal components to obtain the reduced-dimensional representation of the data. This transformed dataset will have fewer dimensions, where each dimension represents a principal component.\n",
    "\n",
    "By applying PCA, you can effectively reduce the dimensionality of the dataset while retaining the most important information or patterns present in the original data. This reduction in dimensions is beneficial for building models to predict stock prices as it helps to overcome the curse of dimensionality, reduces computational complexity, and can improve model performance by focusing on the most informative features.\n",
    "\n",
    "After reducing the dimensionality using PCA, you can use the transformed dataset as input to train your stock price prediction model, leveraging the reduced set of principal components instead of the original numerous features.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81fc6a29-a2eb-404d-8a79-ab64da959379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_77/1348310909.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  dataset = np.array([\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#Standardize the dataset:\u001b[39;00m\n\u001b[1;32m     17\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m---> 18\u001b[0m scaled_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"Standardizing the data ensures that each feature has zero mean and unit variance, which is necessary for PCA.\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#Perform PCA:\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 142\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    147\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    148\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 142\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    147\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    148\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:848\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    847\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 848\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    851\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:824\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 824\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:861\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m    860\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 861\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    868\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 535\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:877\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    875\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 877\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    880\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    881\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    182\u001b[0m     xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(array)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy.array_api\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# Q6complete data is not given in question so I request you to ignore the Errors found\n",
    "# I just mentioned the steps as \n",
    "\"To use PCA (Principal Component Analysis) for dimensionality reduction in a stock price prediction project, you can follow these steps:\"\n",
    "\n",
    "#Import the necessary libraries:\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#Prepare the  dataset:\n",
    "# Assume you have a dataset with stock features, stored in a numpy array called 'dataset'\n",
    "dataset = np.array([\n",
    "    [0.5, 0.2, 0.3, 0.8, 0.9, ...],\n",
    "    [0.4, 0.1, 0.6, 0.2, 0.7, ...],\n",
    "    ...\n",
    "])\n",
    "#Standardize the dataset:\n",
    "scaler = StandardScaler()\n",
    "scaled_dataset = scaler.fit_transform(dataset)\n",
    "\"\"\"Standardizing the data ensures that each feature has zero mean and unit variance, which is necessary for PCA.\"\"\"\n",
    "\n",
    "#Perform PCA:\n",
    "pca = PCA(n_components=0.95)  # Set the desired explained variance threshold (e.g., 95%)\n",
    "reduced_dataset = pca.fit_transform(scaled_dataset)\n",
    "#By specifying n_components=0.95, we choose to retain enough principal components to explain at least 95% of the variance in the data. You can adjust this threshold based on your specific requirements.\n",
    "\n",
    "#Access the selected components and explained variance ratio:\n",
    "selected_components = pca.components_  # Selected principal components\n",
    "explained_variance_ratio = pca.explained_variance_ratio_  # Explained variance ratio of each component\n",
    "\"\"\"The selected_components variable will contain the eigenvectors (principal components) that capture the most important information in the dataset. The explained_variance_ratio variable will provide the percentage of variance explained by each component.\"\"\"\n",
    "\n",
    "\"\"\"Interpret the results:\n",
    "You can analyze the explained_variance_ratio to understand how much variance is captured by each component and decide on the number of components to retain based on your desired threshold.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dee48a1d-cb3d-492b-9cb5-8bf24ff7a4ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To perform Min-Max scaling on the dataset [1, 5, 10, 15, 20] and transform the values to a range of -1 to 1, we can follow these steps:\\n\\nCalculate the minimum and maximum values in the dataset. In this case, the minimum value is 1 and the maximum value is 20.\\n\\nApply the Min-Max scaling formula for each value in the dataset:\\n\\nscaled_value = (original_value - min_value) / (max_value - min_value) * (new_max - new_min) + new_min\\n\\nIn this case, the new_min is -1 and the new_max is 1.\\n\\nPerform the calculation for each value:\\n\\nFor the value 1:\\nscaled_value = (1 - 1) / (20 - 1) * (1 - (-1)) + (-1)\\n= 0 / 19 * 2 + (-1)\\n= -1\\n\\nFor the value 5:\\nscaled_value = (5 - 1) / (20 - 1) * (1 - (-1)) + (-1)\\n= 4 / 19 * 2 + (-1)\\n= -0.1053\\n\\nFor the value 10:\\nscaled_value = (10 - 1) / (20 - 1) * (1 - (-1)) + (-1)\\n= 9 / 19 * 2 + (-1)\\n= -0.2632\\n\\nFor the value 15:\\nscaled_value = (15 - 1) / (20 - 1) * (1 - (-1)) + (-1)\\n= 14 / 19 * 2 + (-1)\\n= 0.2632\\n\\nFor the value 20:\\nscaled_value = (20 - 1) / (20 - 1) * (1 - (-1)) + (-1)\\n= 19 / 19 * 2 + (-1)\\n= 1\\n\\nThe Min-Max scaled values for the dataset [1, 5, 10, 15, 20] within the range of -1 to 1 are:\\n[-1, -0.5789,0.052631578947368474, 0.4736842105263157, 1.0]\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# question 7\n",
    "\"\"\"To perform Min-Max scaling on the dataset [1, 5, 10, 15, 20] and transform the values to a range of -1 to 1, we can follow these steps:\n",
    "\n",
    "Calculate the minimum and maximum values in the dataset. In this case, the minimum value is 1 and the maximum value is 20.\n",
    "\n",
    "Apply the Min-Max scaling formula for each value in the dataset:\n",
    "\n",
    "scaled_value = (original_value - min_value) / (max_value - min_value) * (new_max - new_min) + new_min\n",
    "\n",
    "In this case, the new_min is -1 and the new_max is 1.\n",
    "\n",
    "Perform the calculation for each value:\n",
    "\n",
    "For the value 1:\n",
    "scaled_value = (1 - 1) / (20 - 1) * (1 - (-1)) + (-1)\n",
    "= 0 / 19 * 2 + (-1)\n",
    "= -1\n",
    "\n",
    "For the value 5:\n",
    "scaled_value = (5 - 1) / (20 - 1) * (1 - (-1)) + (-1)\n",
    "= 4 / 19 * 2 + (-1)\n",
    "= -0.1053\n",
    "\n",
    "For the value 10:\n",
    "scaled_value = (10 - 1) / (20 - 1) * (1 - (-1)) + (-1)\n",
    "= 9 / 19 * 2 + (-1)\n",
    "= -0.2632\n",
    "\n",
    "For the value 15:\n",
    "scaled_value = (15 - 1) / (20 - 1) * (1 - (-1)) + (-1)\n",
    "= 14 / 19 * 2 + (-1)\n",
    "= 0.2632\n",
    "\n",
    "For the value 20:\n",
    "scaled_value = (20 - 1) / (20 - 1) * (1 - (-1)) + (-1)\n",
    "= 19 / 19 * 2 + (-1)\n",
    "= 1\n",
    "\n",
    "The Min-Max scaled values for the dataset [1, 5, 10, 15, 20] within the range of -1 to 1 are:\n",
    "[-1, -0.5789,0.052631578947368474, 0.4736842105263157, 1.0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bdd6d14-73e0-4bdc-9ab4-05715a961f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0, -0.5789473684210527, -0.052631578947368474, 0.4736842105263157, 1.0]\n"
     ]
    }
   ],
   "source": [
    "#Q7 code:-\n",
    "import numpy as np\n",
    "def min_max_scaling(data,new_min,new_max):\n",
    "    min_val=min(data)\n",
    "    max_val=max(data)\n",
    "    \n",
    "    scaled_data=[(value-min_val)/(max_val-min_val)*(new_max-new_min)+new_min for value in data]\n",
    "    return scaled_data\n",
    "\n",
    "original_data=[1, 5, 10, 15, 20]\n",
    "scaled_data=min_max_scaling(original_data,-1,1)\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1246365c-de91-4f2f-acca-c84d17837786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To perform feature extraction using PCA on a dataset with features [height, weight, age, gender, blood pressure], the number of principal components to retain depends on the desired level of explained variance and the specific requirements of the application. Here\\'s an overview of the process:\\n\\n1.Prepare the dataset: Ensure that the dataset is properly prepared, with any necessary preprocessing steps like handling missing values or encoding categorical variables.\\n\\n2.Standardize the data: Since PCA is affected by the scale of the features, it is important to standardize the dataset so that all features have zero mean and unit variance.\\n\\n3.Perform PCA: Apply PCA on the standardized dataset to identify the principal components.\\n\\nDetermine the number of principal components to retain: This decision can be based on the explained variance ratio or eigenvalues associated with each principal component. The explained variance ratio indicates the proportion of variance in the dataset explained by each principal component. Choosing the number of principal components involves a trade-off between retaining enough variance to capture important information and reducing the dimensionality of the dataset. Common approaches include setting a threshold for the cumulative explained variance ratio (e.g., retaining components that explain 90% or more of the variance) or examining the scree plot to identify the \"elbow\" point where the eigenvalues drop significantly.\\n\\nIt\\'s difficult to determine the exact number of principal components to retain without analyzing the dataset. However, here are some considerations:\\n\\na)Explained Variance: Look at the cumulative explained variance ratio. If, for example, the first few components explain a significant portion of the variance (e.g., 90% or more), you may consider retaining those components.\\n\\nb)Dimensionality Reduction: If one of the goals is to reduce dimensionality, you can choose a smaller number of principal components that still capture a reasonable amount of variance. Keep in mind that reducing dimensionality comes at the cost of potentially losing some information.\\n\\nc)Application-specific Requirements: Consider the specific requirements of your application. If certain features are more important or if there are domain-specific constraints, you may prioritize retaining components that capture relevant information.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q8\n",
    "\"\"\"To perform feature extraction using PCA on a dataset with features [height, weight, age, gender, blood pressure], the number of principal components to retain depends on the desired level of explained variance and the specific requirements of the application. Here's an overview of the process:\n",
    "\n",
    "1.Prepare the dataset: Ensure that the dataset is properly prepared, with any necessary preprocessing steps like handling missing values or encoding categorical variables.\n",
    "\n",
    "2.Standardize the data: Since PCA is affected by the scale of the features, it is important to standardize the dataset so that all features have zero mean and unit variance.\n",
    "\n",
    "3.Perform PCA: Apply PCA on the standardized dataset to identify the principal components.\n",
    "\n",
    "Determine the number of principal components to retain: This decision can be based on the explained variance ratio or eigenvalues associated with each principal component. The explained variance ratio indicates the proportion of variance in the dataset explained by each principal component. Choosing the number of principal components involves a trade-off between retaining enough variance to capture important information and reducing the dimensionality of the dataset. Common approaches include setting a threshold for the cumulative explained variance ratio (e.g., retaining components that explain 90% or more of the variance) or examining the scree plot to identify the \"elbow\" point where the eigenvalues drop significantly.\n",
    "\n",
    "It's difficult to determine the exact number of principal components to retain without analyzing the dataset. However, here are some considerations:\n",
    "\n",
    "a)Explained Variance: Look at the cumulative explained variance ratio. If, for example, the first few components explain a significant portion of the variance (e.g., 90% or more), you may consider retaining those components.\n",
    "\n",
    "b)Dimensionality Reduction: If one of the goals is to reduce dimensionality, you can choose a smaller number of principal components that still capture a reasonable amount of variance. Keep in mind that reducing dimensionality comes at the cost of potentially losing some information.\n",
    "\n",
    "c)Application-specific Requirements: Consider the specific requirements of your application. If certain features are more important or if there are domain-specific constraints, you may prioritize retaining components that capture relevant information.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f99986a-0a79-4a1f-922e-ba4d15e8c6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio: [8.57304268e-01 1.21307577e-01 2.11456304e-02 2.42524254e-04\n",
      " 1.19549903e-34]\n",
      "Scree plot: [0.85730427 0.97861185 0.99975748 1.         1.        ]\n",
      "Number of components to retain: 2\n",
      "Reduced dataset:\n",
      "[[ 0.30539129  1.23781941]\n",
      " [ 1.15295494 -1.07462049]\n",
      " [-1.94492609  0.29857699]\n",
      " [ 3.08132801  0.04289719]\n",
      " [-2.59474815 -0.5046731 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Original dataset\n",
    "dataset = np.array([\n",
    "    [170, 65, 30, 1, 120],\n",
    "    [165, 60, 35, 0, 130],\n",
    "    [180, 70, 40, 1, 140],\n",
    "    [160, 55, 25, 0, 110],\n",
    "    [175, 75, 45, 1, 150]\n",
    "])\n",
    "\n",
    "# Standardize the dataset\n",
    "scaler = StandardScaler()\n",
    "scaled_dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "pca.fit(scaled_dataset)\n",
    "\n",
    "# Explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Scree plot\n",
    "scree_plot = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Determine the number of components to retain\n",
    "num_components = np.argmax(scree_plot >= 0.9) + 1\n",
    "\n",
    "# Perform dimensionality reduction\n",
    "reduced_dataset = pca.transform(scaled_dataset)[:, :num_components]\n",
    "\n",
    "# Print the results\n",
    "print(\"Explained variance ratio:\", explained_variance_ratio)\n",
    "print(\"Scree plot:\", scree_plot)\n",
    "print(\"Number of components to retain:\", num_components)\n",
    "print(\"Reduced dataset:\")\n",
    "print(reduced_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0736771-73a8-433b-b186-15a579dfc15e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439740c9-f3fc-4ef7-b359-910f97122970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da611db7-3a95-4576-8bfd-753dcfc4cd51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb12ca95-f7ae-4f0a-9a6f-5657e8a4d2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b343c673-f5e7-4fb3-bf5a-526302af8971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c4b0b3-7489-4a4d-9977-0708e59507d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e3ddef-d80a-47ec-9618-b4f375cafaf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ddba6d-6dc5-4566-8fb1-2145cb270878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d104391-f02d-4bb9-b28e-b6fa07700358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3dcb52-5f2d-4e15-9d01-25962d6a7684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa74d84-70b5-4c39-b43d-95c11cdf3ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97037f28-62fa-468d-8a89-1de9d8b7ade6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d973b009-f242-4ad5-93e1-6d789be7f8ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe8090f-4fe2-4987-8be1-bade61a7b4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf10d56-cbb9-461e-99d8-362437241c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dbc9be-b7a1-4dd3-9469-94e5149fad1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019473b5-b48d-4fc5-a857-7cecad8051e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30afd60e-6f6f-4489-810a-373bc96d9a61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8561dc1-7d7b-4519-8885-93ece1f70698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f389b52-f3a3-4499-b323-e7061df1393a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65ecd04-7f29-459f-99c9-f8c69451f1c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6452ada7-f0ac-4ec6-97c0-a7515a433852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4586df7d-0dae-4809-bc31-fd53bc79a25a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c622a6e-34ee-4c88-8253-a3b9734ed51a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e3fbb9-943d-4827-a102-e2bcf7fcc1be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a832e69-1920-4c5e-b9e1-2326ab3437b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b79685-e527-4e52-9558-7f3dcea209b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48eb7c5-c066-4246-8cda-89aca77f6960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0250493-d6c0-4c08-846e-874c5372a2cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e0997d-1b4c-44e5-a4a8-8b4258ec3ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f2e5a9-1daa-4b49-a8ea-b69a05a23824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e141a7-e767-4e08-94d0-2f1d9c216f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209317bd-00b3-43e2-b951-8f4b8b81913f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10da8be8-f222-4e76-9940-7ae87fcb028f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae264af6-242b-45c6-a11e-b6b31d4cb964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963b4cb4-406b-4f21-ac8f-b67493f49010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb37cdd-33b5-42f2-9658-a1350dcfd66e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972aaaba-aad5-492f-ba6d-a41340b9c7b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec04b2d2-d27b-4285-8b72-612f77519e16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498af9c1-e95e-440d-8330-36d5ba65552a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c182a625-f808-470e-b2c7-48d7a043b24a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4bb862-a979-4ed2-a79b-7921f75af1bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3277f9ce-611b-45ea-87ab-0fd83b7c0d27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8613c203-a458-4ab6-9bfa-3da41541f0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0888240-2efd-4057-80cc-3e10f87a644a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79592a5-494e-4049-bdec-c911fbd42e39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad27c645-77c6-4c6e-b80e-41bf8557b735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9da6e8-792f-48fd-b035-e403a0b64a39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b8c59a-583c-4020-bcfe-be0bf24e485b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fc897a-47d5-4493-82af-4097511cd4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bb1a20-6928-467f-bf5e-806a5e86eb3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4624a1-2e70-4e88-b006-6b4cfdb1fb20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db02c40-f5fb-4b6a-aa08-2bd5acbe5071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb46b9b2-d862-44c8-9f92-370bdc862646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3e64d5-4656-481e-be79-b8b7410d5f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d68ee8-7983-4030-8baa-e376238c7e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ab8526-7a48-4a21-8cc1-e5700fe14a70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbfc7a3-2012-4163-a8de-3401aa9dbc21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74f86ce-fa1e-4c88-a8ec-e91e1d0b61a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d800394-6ec7-438a-832f-55679350ffb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ab9cba-39f8-4b9b-baa3-44090baad53d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
